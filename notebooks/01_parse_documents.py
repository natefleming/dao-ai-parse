# Databricks notebook source
from typing import Sequence

pip_requirements: Sequence[str] = (
  "dao-ai",
  "databricks-vectorsearch",
  "docling",
  "langchain-text-splitters",
  "llama-index",
  "mlflow",
  "pypdf",
  "tiktoken",
  "torch",
  "transformers"
)

pip_requirements: str = " ".join(pip_requirements)

%pip install --quiet --upgrade {pip_requirements}
%pip uninstall --quiet -y databricks-connect pyspark pyspark-connect
%pip install --quiet databricks-connect
dbutils.library.restartPython()

# COMMAND ----------

from importlib.metadata import version

print(f"dao-ai=={version('dao-ai')}")
print(f"databricks-vectorsearch=={version('databricks-vectorsearch')}")
print(f"docling=={version('docling')}")
print(f"langchain=={version('langchain')}")
print(f"llama-index=={version('llama-index')}")
print(f"mlflow=={version('mlflow')}")
print(f"pypdf=={version('pypdf')}")
print(f"tiktoken=={version('tiktoken')}")
print(f"torch=={version('torch')}")
print(f"transformers=={version('transformers')}")

# COMMAND ----------

from pathlib import Path
from rich import print as pprint

dbutils.widgets.text(name="config-path", defaultValue="../config/example.yaml")
config_path: str = dbutils.widgets.get("config-path")

dbutils.widgets.text(name="vector-store-alias", defaultValue="documents_vector_store")
vector_store_alias: str = dbutils.widgets.get("vector-store-alias")

dbutils.widgets.text(name="chunk-size", defaultValue="1024")
chunk_size: int = int(dbutils.widgets.get("chunk-size"))

dbutils.widgets.text(name="chunk-overlap", defaultValue="128")
chunk_overlap: int = int(dbutils.widgets.get("chunk-overlap"))

dbutils.widgets.text(name="records-per-batch", defaultValue="10")
records_per_batch: int = int(dbutils.widgets.get("records-per-batch"))

if not Path(config_path).exists():
  raise ValueError(f"Config file not found: {config_path}")

pprint(f"{config_path=}")
pprint(f"{vector_store_alias=}")
pprint(f"{chunk_size=}")
pprint(f"{chunk_overlap=}")
pprint(f"{records_per_batch=}")

# COMMAND ----------

from dao_ai.config import AppConfig


app: AppConfig = AppConfig.from_file(config_path)

# COMMAND ----------

from dao_ai.config import SchemaModel

for _, schema_model in app.schemas.items():
  schema_model: SchemaModel
  schema_model.create()


# COMMAND ----------

from dao_ai.config import VectorStoreModel

if vector_store_alias not in app.resources.vector_stores:
  raise ValueError(f"Vector store not found: {vector_store_alias}")

vector_store_model: VectorStoreModel = app.resources.vector_stores[vector_store_alias]

pprint(vector_store_model)

# COMMAND ----------

from pathlib import Path
from dao_ai.config import VolumePathModel

source_path_model: VolumePathModel = vector_store_model.source_path
source_path_model.create()

checkpoints_path_model: VolumePathModel = vector_store_model.checkpoint_path
checkpoints_path_model.create()

source_documents_path: Path = Path(source_path_model.full_name)
source_checkpoint_path: Path = Path(checkpoints_path_model.full_name)

pprint(source_documents_path)
pprint(source_checkpoint_path)

# COMMAND ----------


source_table_name: str = vector_store_model.source_table.full_name
document_uri: str = "source"

spark.sql(f"""
  CREATE TABLE IF NOT EXISTS {source_table_name} (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    {document_uri} STRING,
    mime_type STRING,
    content STRING
  ) TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

# COMMAND ----------

from typing import Callable, Iterator
from io import BytesIO
import warnings
import mimetypes
import json
import uuid

from pyspark.sql import DataFrame
import pyspark.sql.functions as F
import pyspark.sql.types as T

from docling.document_converter import DocumentConverter, ConversionResult
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.document_converter import (
    DocumentConverter,
    PdfFormatOption,
    WordFormatOption,
)
from docling.pipeline.simple_pipeline import SimplePipeline
from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling_core.types.doc.document import DoclingDocument

pipeline_options: PdfPipelineOptions = PdfPipelineOptions()
pipeline_options.do_ocr = False
pipeline_options.do_table_structure = True

from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Document, set_global_tokenizer

from transformers import AutoTokenizer

import pandas as pd


## Custom options are now defined per format.
def document_converter() -> DocumentConverter:
    converter: DocumentConverter = (
        DocumentConverter(  # all of the below is optional, has internal defaults.
            allowed_formats=[
                InputFormat.PDF,
                InputFormat.IMAGE,
                InputFormat.DOCX,
                InputFormat.HTML,
                InputFormat.PPTX,
            ],  # whitelist formats, non-matching files are ignored.
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options, # pipeline options go here.
                    backend=PyPdfiumDocumentBackend # optional: pick an alternative backend
                ),
                InputFormat.DOCX: WordFormatOption(
                    pipeline_cls=SimplePipeline # default for office formats and HTML
                ),
            },
        )
    )
    return converter


def parse_bytes(raw_doc_contents_bytes: bytes, name: str, converter: DocumentConverter) -> str:
    try:
      stream: BytesIO = BytesIO(raw_doc_contents_bytes)
      document_stream: DocumentStream = DocumentStream(name=name, stream=stream)
      result: ConversionResult = converter.convert(document_stream)
      document: DoclingDocument = result.document
      markdown: str = document.export_to_markdown()
      return markdown
    except Exception as e:
        warnings.warn(f"Exception {e} has been thrown during parsing")
        return None


def guess_mime_type_factory() -> Callable[[str], str]:
    @F.udf(T.StringType())
    def guess_mime_type(path: str) -> str:
        return mimetypes.guess_type(path)[0]
  
    return guess_mime_type

def read_as_chunk_factory(chunk_size: int, chunk_overlap: int) -> Callable[[pd.Series], pd.Series]:
    @F.pandas_udf("array<string>")
    def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:
        # set llama2 as tokenizer to match our model size (will stay below gte 1024 limit)
        set_global_tokenizer(
            AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")
        )

        splitter: SentenceSplitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        converter: DocumentConverter = document_converter()

        def extract_and_split(b: bytes):
            name: str = f"doc_{uuid.uuid4().hex[:8]}"
            text: str = parse_bytes(raw_doc_contents_bytes=b, name=name, converter=converter)
            if text is None:
                return []
            nodes = splitter.get_nodes_from_documents([Document(text=text)])
            return [n.text for n in nodes]

        for x in batch_iter:
            yield x.apply(extract_and_split)
  
    return read_as_chunk


# COMMAND ----------

from typing import Callable, Iterator
from pyspark.sql import DataFrame
import pandas as pd


spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", records_per_batch)

document_uri: str = "source"

df: DataFrame = (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "BINARYFILE")
    .load(source_documents_path.as_posix())
)

guess_mime_type: Callable[[str], str] = guess_mime_type_factory()
read_as_chunk: Callable[[Iterator[pd.Series]], Iterator[pd.Series]] = (
    read_as_chunk_factory(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
)

(
    df
    .withColumn("mime_type", guess_mime_type(df.path))
    .withColumn("content", F.explode(read_as_chunk("content")))
    .selectExpr(f"path as {document_uri}", "mime_type", "content")
    .writeStream.trigger(availableNow=True)
    .option("checkpointLocation", source_checkpoint_path.as_posix())
    .table(source_table_name)
    .awaitTermination()
)

# COMMAND ----------

display(spark.table(source_table_name))
